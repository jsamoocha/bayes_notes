{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing Bayes' Rule\n",
    "\n",
    "Recall our second problem from [the introduction](./01_bayes_rule_intro.ipynb#Motivating-Examples---Limitations-of-Machine-Learning-and-(frequentist)-Statistics): we observe some heart rate measurements from a workout of a given person. We want to know which heart rate interval contains the true mean for this person with 95% _probability_ a.k.a. the **credible interval** (this is _not_ the same as a 95% confidence interval, see [this thorough explanation](http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/)). We'd also like to know how prior knowledge (e.g. some known population mean and variance) can influence our inference of this interval.\n",
    "\n",
    "For this, we need to generalize Bayes' rule a bit. Let $y = y_1,\\ldots,y_n$ be the data we observe, and $\\theta$ a parametric **model** representing some **process that generates the data $y$**. We are now interested in $p(\\theta\\mid y)$, i.e. the probability (density!) of a model, given the observed data. Bayes' rule can now be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 3em\">\n",
    "$$\n",
    "p(\\theta\\mid y) = \\frac{p(y\\mid\\theta)p(\\theta)}{p(y)}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of this equation should be interpreted as follows:\n",
    "\n",
    "- a **likelihood** $p(y\\mid\\theta)$, i.e. the likelihood of observing $y$, given a particular model $\\theta$,\n",
    "- a **prior distribution** $p(\\theta)$, i.e. the distribution of possible parameter values of the model $\\theta$,\n",
    "- a **marginal likelihood** $p(y)$, i.e. the joint distribution $p(y, \\theta)$, with $\\theta$ integrated out: $\\int_\\theta p(y\\mid\\theta)p(\\theta)\\,\\mathrm{d}x$. This component is usually ignored,\n",
    "- a **posterior distribution** $p(\\theta\\mid y)$, i.e. the distribution of $\\theta$, given its prior and after having observed the data $y$;\n",
    "\n",
    "To reiterate: let it sink in that the prior and posterior distributions are (continuous) **distributions of model parameters**.\n",
    "\n",
    "Or, in other words, a model (made up of one or more model parameters) can be seen as just a random variable. It is not fixed at a given value or state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example, observing a single value with a simple model\n",
    "\n",
    "If we assume that the _process of generating values_ $y_i$ is defined as sampling from a Normal distribution with unknown mean $\\mu$ and variance $\\sigma^2$: $y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, $\\theta = \\{\\mu, \\sigma^2\\}$. In this case, Bayes' rule looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "p(\\mu, \\sigma^2\\mid y_1,\\ldots,y_n) = \\frac{p(y_1,\\ldots,y_n\\mid\\mu, \\sigma^2)p(\\mu,\\sigma^2)}{p(y_1,\\ldots,y_n)}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems a bit intimidating, so let's simplify the example by assuming just one observation, $y=170$ bpm. The model $\\theta$ can be simplified by considering the variance $\\sigma^2$ to be fixed, e.g. at 100 bpm. Also assume we prviously observed a large sample of heart rates with a mean of 130 bpm and variance of 80 from a diverse population. A full specification of the model now looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu &\\sim\\color{red}{\\mathcal{N}(m_0, s_0^2)}\\,\\mathrm{(prior)}\\\\\n",
    "y &\\sim\\color{blue}{\\mathcal{N}(\\mu, \\sigma^2_0)}\\,\\mathrm{(likelihood)}\\\\\n",
    "m_0 &=130\\\\\n",
    "s_0^2 &=80\\\\\n",
    "\\sigma^2_0 &=100\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mu$ is a random variable, $y$ is a random variable for which we have observations (n=1), and $m_0$, $s_0^2$ and $\\sigma_0^2$ are **hyperparameters** (these are fixed before doing inference). This makes it possible to compute the posterior distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mu\\mid y) &= \\frac{\\color{blue}{p(y\\mid\\mu)}\\color{red}{p(\\mu)}}{p(y)}\\\\\n",
    "&= \\frac{\\color{blue}{p(y\\mid\\mu)}\\color{red}{p(\\mu)}}{\\int_{-\\infty}^{\\infty}p(y\\mid\\mu)p(\\mu)\\,\\mathrm{d}\\mu}\\\\\n",
    "&\\propto \\color{blue}{p(y\\mid\\mu)}\\color{red}{p(\\mu)}\\,\\,(1)\\\\\n",
    "&=\\color{blue}{\\frac{1}{\\sqrt{2\\pi\\color{black}{\\sigma_0^2}}}exp\\left\\{-\\frac{1}{2\\color{black}{\\sigma_0^2}}{(\\color{black}{y} - \\mu)}^2\\right\\}}\\color{red}{\\frac{1}{\\sqrt{2\\pi \\color{black}{s_0^2}}}exp\\left\\{-\\frac{1}{2 \\color{black}{s_0^2}}{(\\mu - \\color{black}{m_0})}^2\\right\\}}\\,\\,(2)\\\\\n",
    "&\\propto \\ldots\\,\\,(3)\\\\\n",
    "&\\propto exp\\left\\{-\\frac{1}{2 \\color{orange}{{\\left(\\frac{1}{s_0^2} + \\frac{1}{\\sigma_0^2}\\right)}^{-1}}}{\\left(\\mu - \\color{green}{{\\left(\\frac{1}{s_0^2} + \\frac{1}{\\sigma_0^2}\\right)}^{-1}\\left(\\frac{m_0}{s_0^2} + \\frac{y}{\\sigma_0^2}\\right)}\\right)}^2\\right\\}\\\\\n",
    "&= exp\\left\\{-\\frac{1}{2 \\color{orange}{s_1^2}}{(\\mu - \\color{green}{m1})}^2\\right\\}\\,\\,(4)\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result at (4) is, apart from some constant, equal to the normal PDF. The denominator in (1) evaluates to a constant that makes the numerator a proper PDF. Since the normal PDF integrates to 1, we can don't need to compute the integral in the denominator in (1).\n",
    "\n",
    "Be aware that the expression (2) looks very complicated, but actually has only 1 variable, i.e. $\\mu$.\n",
    "\n",
    "For those interested in the rewriting magic that leads to the result, look [here](http://www.ams.sunysb.edu/~zhu/ams570/Bayesian_Normal.pdf).\n",
    "\n",
    "Given our likelihood and priors as defined above, the posterior distribution becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\mu\\mid y \\sim \\mathcal{N}(m_1, s_1^2),\\\\\n",
    "s_1^2 = {\\left(\\frac{1}{s_0^2} + \\frac{1}{\\sigma_0^2}\\right)}^{-1}\\\\\n",
    "m_1 = s_1^2\\left(\\frac{m_0}{s_0^2} + \\frac{y}{\\sigma_0^2}\\right)\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def to_posterior(y, likelihood_variance, prior):\n",
    "    posterior_variance = 1 / ((1/prior.var()) + (1/likelihood_variance))\n",
    "    posterior_mean = posterior_variance * ((prior.mean() / prior.var()) + (y / likelihood_variance))\n",
    "    posterior = norm(loc=posterior_mean, scale=np.sqrt(posterior_variance))\n",
    "    setattr(posterior, 'name', 'posterior_{}_{}'.format(prior.mean(), prior.var()))\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "def _dist_range(distributions, n=100):\n",
    "    ranges = [dist.ppf([0.001, 0.999]) for dist in distributions]\n",
    "    return np.linspace(\n",
    "        min((r[0] for r in ranges)),\n",
    "        max((r[1] for r in ranges)),\n",
    "        n\n",
    "    )\n",
    "\n",
    "def plot_density(distributions):\n",
    "    x = _dist_range(distributions)\n",
    "    return go.FigureWidget(\n",
    "        data=[\n",
    "            go.Scatter(x=x, y=dist.pdf(x), mode='lines', line={'shape': 'spline', 'width': 4}, fill='tozeroy', name=dist.name)\n",
    "            for dist in distributions\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = norm(loc=130, scale=10)\n",
    "setattr(prior, 'name', 'prior_130_1')\n",
    "\n",
    "plot_density([prior, to_posterior(170, 100, prior)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
