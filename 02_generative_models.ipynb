{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing Bayes' Rule\n",
    "\n",
    "Recall our second problem from [the introduction](./01_bayes_rule_intro.ipynb#Motivating-Examples---Limitations-of-Machine-Learning-and-(frequentist)-Statistics): we observe some heart rate measurements from a workout of a given person. We want to know which heart rate interval contains the true mean for this person with 95% _probability_ a.k.a. the **credible interval** (this is _not_ the same as a 95% confidence interval, see [this thorough explanation](http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/)). We'd also like to know how prior knowledge (e.g. some known population mean and variance) can influence our inference of this interval.\n",
    "\n",
    "For this, we need to generalize Bayes' rule a bit. Let $y = y_1,\\ldots,y_n$ be the data we observe, and $\\theta$ a parametric **model** representing some **process that generates the data $y$**. We are now interested in $p(\\theta\\mid y)$, i.e. the probability (density!) of a model, given the observed data. Bayes' rule can now be written as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 3em\">\n",
    "$$\n",
    "p(\\theta\\mid y) = \\frac{p(y\\mid\\theta)p(\\theta)}{p(y)}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components of this equation should be interpreted as follows:\n",
    "\n",
    "- a **likelihood** $p(y\\mid\\theta)$, i.e. the likelihood of observing $y$, given a particular model $\\theta$,\n",
    "- a **prior distribution** $p(\\theta)$, i.e. the distribution of possible parameter values of the model $\\theta$,\n",
    "- a **marginal likelihood** $p(y)$, i.e. the joint distribution $p(y, \\theta)$, with $\\theta$ integrated out: $\\int_\\theta p(y\\mid\\theta)p(\\theta)\\,\\mathrm{d}x$. This component is usually ignored,\n",
    "- a **posterior distribution** $p(\\theta\\mid y)$, i.e. the distribution of $\\theta$, given its prior and after having observed the data $y$;\n",
    "\n",
    "To reiterate: the prior and posterior distributions are (continuous) **distributions of model parameters**.\n",
    "\n",
    "Or, in other words, a model (made up of one or more model parameters) can be seen as just a random variable. It is not fixed at a given value or state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example, observing a single value with a simple model\n",
    "\n",
    "If we assume that the _process of generating values_ $y_i$ is defined as sampling from a Normal distribution with unknown mean $\\mu$ and variance $\\sigma^2$: $y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, $\\theta = \\{\\mu, \\sigma^2\\}$. In this case, Bayes' rule looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "p(\\mu, \\sigma^2\\mid y_1,\\ldots,y_n) = \\frac{p(y_1,\\ldots,y_n\\mid\\mu, \\sigma^2)p(\\mu,\\sigma^2)}{p(y_1,\\ldots,y_n)}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems a bit intimidating, so let's simplify the example by assuming just one observation, $y=170$ bpm. The model $\\theta$ can be simplified by considering the variance $\\sigma^2$ to be fixed, e.g. at 100 bpm. Also assume we prviously observed a large sample of heart rates with a mean of 130 bpm and variance of 80 from a diverse population. A full specification of the model now looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu &\\sim\\color{red}{\\mathcal{N}(m_0, s_0^2)}\\,\\mathrm{(prior)}\\\\\n",
    "y &\\sim\\color{blue}{\\mathcal{N}(\\mu, \\sigma^2_0)}\\,\\mathrm{(likelihood)}\\\\\n",
    "m_0 &=130\\\\\n",
    "s_0^2 &=80\\\\\n",
    "\\sigma^2_0 &=100\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mu$ is a random variable, $y$ is a random variable for which we have observations (n=1), and $m_0$, $s_0^2$ and $\\sigma_0^2$ are **hyperparameters** (these are fixed before doing inference). This makes it possible to compute the posterior distribution (note that $\\propto$ means \"proportional to\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mu\\mid y) &= \\frac{\\color{blue}{p(y\\mid\\mu)}\\color{red}{p(\\mu)}}{p(y)}\\\\\n",
    "&= \\frac{\\color{blue}{p(y\\mid\\mu)}\\color{red}{p(\\mu)}}{\\int_{-\\infty}^{\\infty}p(y\\mid\\mu)p(\\mu)\\,\\mathrm{d}\\mu}\\\\\n",
    "&\\propto \\color{blue}{p(y\\mid\\mu)}\\color{red}{p(\\mu)}\\,\\,(1)\\\\\n",
    "&=\\color{blue}{\\frac{1}{\\sqrt{2\\pi\\color{black}{\\sigma_0^2}}}exp\\left\\{-\\frac{1}{2\\color{black}{\\sigma_0^2}}{(\\color{black}{y} - \\mu)}^2\\right\\}}\\color{red}{\\frac{1}{\\sqrt{2\\pi \\color{black}{s_0^2}}}exp\\left\\{-\\frac{1}{2 \\color{black}{s_0^2}}{(\\mu - \\color{black}{m_0})}^2\\right\\}}\\,\\,(2)\\\\\n",
    "&\\propto \\ldots\\,\\,(3)\\\\\n",
    "&\\propto exp\\left\\{-\\frac{1}{2 \\color{orange}{{\\left(\\frac{1}{s_0^2} + \\frac{1}{\\sigma_0^2}\\right)}^{-1}}}{\\left(\\mu - \\color{green}{{\\left(\\frac{1}{s_0^2} + \\frac{1}{\\sigma_0^2}\\right)}^{-1}\\left(\\frac{m_0}{s_0^2} + \\frac{y}{\\sigma_0^2}\\right)}\\right)}^2\\right\\}\\\\\n",
    "&= exp\\left\\{-\\frac{1}{2 \\color{orange}{s_1^2}}{(\\mu - \\color{green}{m1})}^2\\right\\}\\,\\,(4)\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result at (4) is, apart from some constant, equal to the normal PDF. The denominator in (1) evaluates to a constant that makes the numerator a proper PDF. Since the normal PDF integrates to 1, we don't need to compute the integral in the denominator in (1).\n",
    "\n",
    "Be aware that the expression (2) looks very complicated, but actually has only 1 variable, i.e. $\\mu$.\n",
    "\n",
    "For those interested in the rewriting magic hidden in (3) that leads to the result, look [here](http://www.ams.sunysb.edu/~zhu/ams570/Bayesian_Normal.pdf).\n",
    "\n",
    "Given our likelihood and priors as defined above, the posterior distribution becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\mu\\mid y \\sim \\mathcal{N}(m_1, s_1^2),\\\\\n",
    "s_1^2 = {\\left(\\frac{1}{s_0^2} + \\frac{1}{\\sigma_0^2}\\right)}^{-1}\\\\\n",
    "m_1 = s_1^2\\left(\\frac{m_0}{s_0^2} + \\frac{y}{\\sigma_0^2}\\right)\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be easily implemented using [scipy's norm object](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm  # norm is the scipy object representing a normal distribution\n",
    "\n",
    "def to_posterior(y, likelihood_variance, prior):\n",
    "    posterior_variance = 1 / ((1/prior.var()) + (1/likelihood_variance))  # s_1^2 (see above)\n",
    "    posterior_mean = posterior_variance * ((prior.mean() / prior.var()) + (y / likelihood_variance))  # m_1 (see above)\n",
    "    posterior = norm(loc=posterior_mean, scale=np.sqrt(posterior_variance))\n",
    "    setattr(posterior, 'name', f'posterior_{prior.name}')\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for creating an x range for plotting multiple density functions\n",
    "def _dist_range(distributions, n=1000):\n",
    "    ranges = [dist.ppf([0.001, 0.999]) for dist in distributions]\n",
    "    return np.linspace(\n",
    "        min((r[0] for r in ranges)),\n",
    "        max((r[1] for r in ranges)),\n",
    "        n\n",
    "    )\n",
    "\n",
    "def plot_densities(distributions, title=''):\n",
    "    x = _dist_range(distributions)\n",
    "    return go.FigureWidget(\n",
    "        data=[\n",
    "            go.Scatter(x=x, y=dist.pdf(x), mode='lines', line={'shape': 'spline', 'width': 4}, fill='tozeroy', name=dist.name)\n",
    "            for dist in distributions\n",
    "        ],\n",
    "        layout=go.Layout(\n",
    "            title=title\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the influence of the choice of prior on the posterior distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_tight = norm(loc=130, scale=1)\n",
    "setattr(prior_tight, 'name', 'prior_tight')\n",
    "\n",
    "prior_med = norm(loc=130, scale=10)\n",
    "setattr(prior_med, 'name', 'prior_med')\n",
    "\n",
    "prior_flat = norm(loc=130, scale=30)\n",
    "setattr(prior_flat, 'name', 'prior_flat')\n",
    "\n",
    "plot_densities([prior_tight, prior_med, prior_flat], title='Prior Densities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these priors reflect our (un-)certainty about $\\mu$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_densities([to_posterior(170, 100, prior_tight), to_posterior(170, 100, prior_med), to_posterior(170, 100, prior_flat)], title='Posterior Densities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Observations\n",
    "\n",
    "So far, we only considered 1 observation. When dealing with multiple observations, the posterior becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mu\\mid y_1,\\ldots,y_n) &\\propto \\color{blue}{p(y_1,\\ldots,y_n\\mid\\mu)}p(\\mu)\\\\\n",
    "&= \\prod_{i=1}^n \\color{blue}{\\frac{1}{\\sqrt{2\\pi\\color{black}{\\sigma_0^2}}}exp\\left\\{-\\frac{1}{2\\color{black}{\\sigma_0^2}}{(\\color{black}{y_i} - \\mu)}^2\\right\\}}p(\\mu)\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the [chain rule of probability](https://en.wikipedia.org/wiki/Chain_rule_(probability)), the likelihood of all observations $p(y_1,\\ldots,y_n\\mid\\mu)$\n",
    "equals the product of likelihoods of each single observation $y_i$. Keep in mind that this is based on the assumption that every $y_i$ is **independent** from the others.\n",
    "\n",
    "This eventually leads to a similar posterior as for a single observation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\mu\\mid y_1,\\ldots,y_n \\sim \\mathcal{N}(m_1, s_1^2),\\\\\n",
    "s_1^2 = {\\left(\\frac{1}{s_0^2} + \\frac{n}{\\sigma^2}\\right)}^{-1}\\\\\n",
    "m_1 = s_1^2\\left(\\frac{m_0}{s_0^2} + \\frac{n\\bar{y}}{\\sigma^2}\\right)\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $n$ being the size, $\\bar{y}$ the mean, and $\\sigma^2$ the variance of our sample of observations $y_1,\\ldots,y_n$. The posterior computation can be generalized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_posterior(y, prior, likelihood_var=1):\n",
    "    n = len(y)\n",
    "    y_bar = np.mean(y)\n",
    "    y_var = np.var(y) if n > 1 else likelihood_var\n",
    "    posterior_variance = 1 / ((1/prior.var()) + (n/y_var))  # s_1^2 (see above)\n",
    "    posterior_mean = posterior_variance * ((prior.mean() / prior.var()) + (n*y_bar / y_var))  # m_1 (see above)\n",
    "    posterior = norm(loc=posterior_mean, scale=np.sqrt(posterior_variance))\n",
    "    setattr(posterior, 'name', f'posterior_prior={prior.name}_ybar={y_bar:.1f}_yvar={y_var:.1f}_n={n}')\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the \"tight\" prior from the previous example, how is it updated based on multiple measurements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_densities([\n",
    "    prior_tight,\n",
    "    to_posterior([170], prior_tight, likelihood_var=100),\n",
    "    to_posterior(norm.rvs(loc=170, scale=10, size=10), prior_tight),\n",
    "    to_posterior(norm.rvs(loc=170, scale=10, size=100), prior_tight),\n",
    "    to_posterior(norm.rvs(loc=170, scale=10, size=1000), prior_tight),\n",
    "], title='Posteriors for \"tight\" prior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how about the \"medium certainty\" prior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_densities([\n",
    "    prior_med,\n",
    "    to_posterior([170], prior_med, likelihood_var=100),\n",
    "    to_posterior(norm.rvs(loc=170, scale=10, size=10), prior_med),\n",
    "    to_posterior(norm.rvs(loc=170, scale=10, size=100), prior_med),\n",
    "    to_posterior(norm.rvs(loc=170, scale=10, size=1000), prior_med),\n",
    "], title='Posteriors for \"medium\" prior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_med.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.var(loc=170, scale=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_posterior(norm.rvs(loc=170, scale=10, size=10), prior_med).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the posterior variance (much) smaller than that of the prior or the sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Posterior has the Answers\n",
    "\n",
    "Given that we observe some sample of 10 heart rate measurements from an unknown person's first workout, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = norm.rvs(loc=170, scale=10, size=10)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and given the \"medium-certainty\" prior defined above (in which a population mean of 130 BPM, with variance of 100 is assumed), the posterior becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = to_posterior(y, prior_med)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the posterior's mean, variance, and (inverse) [Cumulative Distribution Function (CDF)](https://en.wikipedia.org/wiki/Cumulative_distribution_function), it is now possible to answer questions such as:\n",
    "\n",
    "_What is this user's expected mean heart rate?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What is the probability that this new user's mean heart rate is below 170?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.cdf(170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What is the probability that this new user's mean heart rate is between 165 en 175?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.cdf(175) - posterior.cdf(165)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For which heart rate can we assign a 30% probability that this user's true mean is below it?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.ppf(0.3)  # This is the inverse of the CDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What is the range of heart rates for which there is a 95% probability that it contains this user's true mean heart rate?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'[{posterior.ppf(0.025)} - {posterior.ppf(0.975)}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing ... X\n",
    "\n",
    "So far we only looked at inference of a single (distribution) parameter based on univariate data $y$. But the Bayesian framework is far more flexible. Can we express a simple linear regression? Assume the most basic example, i.e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "y_i = \\beta x_i + \\epsilon_i, \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which could alternatively be written as a likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "y_i\\mid\\beta,\\sigma^2 \\sim \\mathcal{N}(\\beta x_i, \\sigma^2)\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with PDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "p(y_i\\mid\\beta,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left\\{-\\frac{{(y_i - \\beta x_i)}^2}{2\\sigma^2}\\right\\}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be reminded that only $\\beta$ and $\\sigma^2$ are random variables in this expression, $x$ and $y$ are fixed.\n",
    "\n",
    "Using similar techniques as above, it is possible to derive the posterior $p(\\beta,\\sigma^2\\mid y)$, **in some cases(*)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * The \"Different\" Cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, getting the posterior distribution of our parameters of interest involves (depending on the complexity of the model, and number of parameters) a multiplication of 2 or more PDF's, and dividing that by some integral of this product:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\text{posterior} = \\frac{\\text{product of PDF's of likelihood and (many) different priors}}{\\text{the integral over the product of PDF's in the numerator}}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that in the case of a normally distributed likelihood and normally distributed prior, some clever rewriting gives us a normally distributed posterior (and gives us the value of that nasty integral for free).\n",
    "\n",
    "But... what if we choose a different prior? There are many different distributions/PDF's to choose from. Instead of the [example model above](#Example,-observing-a-single-value-with-a-simple-model), the prior for $\\mu$ could have been a [t distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution) with PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "p(\\mu) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi}}{\\left(1 + \\frac{\\mu^2}{\\nu}\\right)}^{-\\frac{\\nu+1}{2}}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\Gamma(x)$ is the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function), or $(x - 1)!$. Taking the product of this PDF and the normal PDF for the likelihood does not result in a well known PDF.\n",
    "\n",
    "What if, in general, $p(y\\mid\\theta)p(\\theta)$ does not evaluate to a known family of probability density? It means we need to compute that integral in the denominator. Which turns out to be intractable in most cases. Are there other means to get the posterior distribution?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
