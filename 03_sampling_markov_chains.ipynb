{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Law of Large Numbers and Monte Carlo Estimation\n",
    "\n",
    "What if there's no well-defined PDF for our product of likehood and prior(s)? It means we cannot use `scipy.stats.norm`, and its convenient `mean()`, `pdf()`, `cdf()`, or `ppf()` functions to establish probabilistic answers to questions about the posterior. Is it possible to answer such questions in a different way? Assuming we have a sample $y_1,\\ldots,y_n$ from an _unknown_ distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.random.normal(loc=130, scale=10, size=10)  # although using samples from a normal distribution, pretend to not know the true distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is not defined by a PDF, but by a **sample**. According to the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), it is possible to approximate the distribution mean with the sample mean, with large enough sample. Is $n=10$ good enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As [mentioned before](./notebooks/01_bayes_rule_intro.ipynb#Continuous:), the probability of $Y$ being between e.g. 110 and 130 is\n",
    "\n",
    "$$Pr(110 \\le Y \\le 130) = \\int_{110}^{130}p(y)$$\n",
    "\n",
    "However, $p(y)$ is undefined and the only thing we have is a sample. But the sample can be used to approximate the integral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cdf(x, sample):\n",
    "    return (sample <= x).sum() / sample.size  # \"integrating\" by counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cdf(130, y) - sample_cdf(110, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be compared with the probability obtained from the true normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "unknown_norm = norm(loc=130, scale=10)\n",
    "unknown_norm.cdf(130) - unknown_norm.cdf(110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples also allow us to compute quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppf(p, sample):\n",
    "    p_index = int(np.round((sample.size - 1) * p))\n",
    "    return np.sort(sample)[p_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the range of $y$ for which the probability is 80% that it contains $y$'s true mean is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'[{sample_ppf(0.1, y)} - {sample_ppf(0.9, y)}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the interval from the true normal distribution would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'[{unknown_norm.ppf(0.1)} - {unknown_norm.ppf(0.9)}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a larger sample leads to more accurate estimations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_large = np.random.normal(loc=130, scale=10, size=10000)\n",
    "\n",
    "{\n",
    "    'mean (130)': np.mean(y_large),\n",
    "    'std (10)': np.std(y_large),\n",
    "    'p_110_130 (0.4772)': sample_cdf(130, y_large) - sample_cdf(110, y_large),\n",
    "    '80% ([117.2 - 142.8])': f'[{sample_ppf(0.1, y_large)} - {sample_ppf(0.9, y_large)}]'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Simulations for Hierarchical Models\n",
    "\n",
    "This process of computing means, variances, probabilities, percentiles (or any other quantities of interest) from simulated samples is called **Monte Carlo Estimation**. When using more complex models, simulations such as above can also be **chained**. For example, consider a model similar to the one in the [previous note](./02_generative_models.ipynb#Example,-observing-a-single-value-with-a-simple-model):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu &\\sim\\color{red}{\\mathcal{t}(m_0, s_0, \\nu)}\\,\\mathrm{(prior)}\\\\\n",
    "y &\\sim\\color{blue}{\\mathcal{N}(\\mu, \\sigma^2_0)}\\,\\mathrm{(likelihood)}\\\\\n",
    "m_0 &=130\\\\\n",
    "s_0 &=10\\\\\n",
    "\\nu &=(n-1)\\,\\text{degrees of freedom for a sample size of}\\,n\\\\\n",
    "\\sigma^2_0 &=100\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint density $p(y, \\mu) = \\color{blue}{p(y\\mid\\mu)}\\color{red}{p(\\mu)}$ cannot be expressed as a known family (e.g. Normal) of probability densities, but can be estimated by simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# basing the prior on a sample of n=10, simulating m=1000 samples\n",
    "mu = t.rvs(df=9, loc=130, scale=10, size=1000)\n",
    "\n",
    "# an array of n location parameters can be plugged in to get n new samples\n",
    "y_chained = norm.rvs(loc=mu, scale=10)\n",
    "\n",
    "{\n",
    "    'mean (130)': np.mean(y_chained),\n",
    "    'std (10)': np.std(y_chained),\n",
    "    'p_110_130 (0.4772)': sample_cdf(130, y_chained) - sample_cdf(110, y_chained),\n",
    "    '80% ([117.2 - 142.8])': f'[{sample_ppf(0.1, y_chained)} - {sample_ppf(0.9, y_chained)}]'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are samples from the **prior predictive** distribution, i.e. given the likelihood and all the priors in a model, what values of $y$ can we expect (before having observed any data)?\n",
    "\n",
    "- Can we use the above approach to sample from the _posterior_ distribution $p(\\mu\\mid y)$? Why (not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains\n",
    "\n",
    "A [**Markov Chain**](https://en.wikipedia.org/wiki/Markov_chain) is a sequence of random variables $X_1,\\ldots,X_n$, with $1,\\ldots,n$ representing points in time, where the probability (density) of $X_{t+1}$ only depends on the value $X_t$, i.e.\n",
    "\n",
    "$$\n",
    "p(X_{t+1}\\mid X_t,X_{t-1},\\ldots,X_2,X_1) = p(X_{t+1}\\mid X_t)\n",
    "$$\n",
    "\n",
    "A random walk with $X_0\\sim\\mathcal{N}(0, 1)$ and $X_{t+1}\\mid X_t\\sim\\mathcal{N}(X_t, 1)$ is an example of a Markov Chain:\n",
    "\n",
    "- Can have a stationary distribution\n",
    "- Random Walk Example\n",
    "- Discrete ref\n",
    "\n",
    "## Getting Picky: the Metropolis Sampler\n",
    "\n",
    "- = a Markov Chain\n",
    "- simplistic assumptions (symmetry)\n",
    "- code example (normal L, t prior)\n",
    "- stationary distribution proven to be target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_random_walk(sigma, mu_init=0):\n",
    "    mu = mu_init\n",
    "    \n",
    "    while True:\n",
    "        yield mu\n",
    "        mu = np.random.normal(loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "random_walk_iter = normal_random_walk(1)\n",
    "n=1000\n",
    "\n",
    "random_walk_trace = go.FigureWidget(\n",
    "    data=[go.Scatter(x=list(range(n)), y=[next(random_walk_iter) for _ in range(n)], showlegend=False)]\n",
    ")\n",
    "\n",
    "random_walk_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with multiple random walks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trace_df = pd.DataFrame()\n",
    "\n",
    "for i in range(10):\n",
    "    random_walk_iter = normal_random_walk(1)  # re-initialize generator for every new random walk\n",
    "    trace = [next(random_walk_iter) for _ in range(n)]\n",
    "    trace_df[f'trace_{i}'] = trace\n",
    "    random_walk_trace.add_scatter(x=list(range(n)), y=trace, showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look ... quite random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with a little tweak of the random walk..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweaked_random_walk(sigma, mu_init=0, phi=0.9):\n",
    "    mu = mu_init\n",
    "    \n",
    "    while True:\n",
    "        yield mu\n",
    "        mu = np.random.normal(loc=phi*mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000\n",
    "sigma=20\n",
    "\n",
    "random_walk_iter = tweaked_random_walk(1, mu_init=np.random.normal(loc=0, scale=sigma))\n",
    "\n",
    "\n",
    "random_walk_trace = go.FigureWidget(\n",
    "    data=[go.Scatter(x=list(range(n)), y=[next(random_walk_iter) for _ in range(n)], showlegend=False)]\n",
    ")\n",
    "\n",
    "random_walk_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df = pd.DataFrame()\n",
    "\n",
    "for i in range(10):\n",
    "    random_walk_iter = tweaked_random_walk(1, mu_init=np.random.normal(loc=0, scale=sigma))  # re-initialize generator for every new random walk\n",
    "    trace = [next(random_walk_iter) for _ in range(n)]\n",
    "    trace_df[f'trace_{i}'] = trace\n",
    "    random_walk_trace.add_scatter(x=list(range(n)), y=trace, showlegend=False)\n",
    "    \n",
    "trace_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the traces converge to a **stationary distribution** (with mean 0). When experimenting with different values for $\\phi$ in the `tweaked_random_walk()` generator, it turns out that the random walks converge in the case of $-1\\lt\\phi\\lt1$. In that case, the generator `tweaked_random_walk()` can be considered a **sampler** for $\\mathcal{N}(0, \\frac{1}{1-\\phi^2})$.\n",
    "\n",
    "Would it be possible to construct such a sampler for arbitrary complex distributions? Such as a posterior $p(\\theta\\mid y)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Picky - The Metropolis Sampler\n",
    "\n",
    "Assume a model that estimates the month-over-month _change_ in mean exercise heart rate $\\mu$ for a population. Such a model can be defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "y &\\sim\\color{blue}{\\mathcal{N}(\\mu, 1)}\\,\\mathrm{(likelihood)}\\\\\n",
    "\\mu &\\sim\\color{red}{\\mathcal{t}(0, 1, 1)}\\,\\mathrm{(prior)}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the prior for $\\mu$ coming from a standard $\\mathcal{t}$ distribution, i.e. centered around 0 with scale equal to 1 standard deviation. This allows to write the posterior as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mu\\mid y_1,\\ldots,y_n) &\\propto \\color{blue}{p(y_1,\\ldots,y_n\\mid\\mu)}\\color{red}{p(\\mu)}\\\\\n",
    "&= \\color{blue}{\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}}exp\\left\\{-\\frac{1}{2}{(\\color{black}{y_i} - \\mu)}^2\\right\\}}\\color{red}{\\frac{1}{\\pi(1+\\mu^2)}}\\\\\n",
    "&\\propto\\ldots\\\\\n",
    "&\\propto \\frac{exp\\left[n(\\bar{y}\\mu - \\frac{\\mu^2}{2})\\right]}{1 + \\mu^2}\\,= g(\\mu)\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $n$ and $\\bar{y}$ are known and fixed. To summarize, $g(\\mu)$ is:\n",
    "\n",
    "- a PDF,\n",
    "- proportional to $p(\\mu\\mid y)$\n",
    "\n",
    "The [Metropolis algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) allows to sample from the posterior $p(\\mu\\mid y)$ by creating a Markov Chain with a stationary distribution that approximates the posterior. It does so by iteratively accepting or rejecting samples drawn from another distribution that is easier to sample. \n",
    "\n",
    "Remember this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.pdf(2) / norm.pdf(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mu\\mid y) &\\propto g(\\mu) \\Leftrightarrow\\\\\n",
    "\\frac{p(\\mu_1\\mid y)}{p(\\mu_2\\mid y)} &= \\frac{g(\\mu_1)}{g(\\mu_2)}\\\\\n",
    "\\text{or}\\\\\n",
    "\\frac{p(\\mu_{t+1}\\mid y)}{p(\\mu_t\\mid y)} &= \\frac{g(\\mu_{t+1})}{g(\\mu_t)}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can evaluate $g(\\mu)$, the ratio $\\frac{g(\\mu_1)}{g(\\mu_2)}$ tells us how much more/less likely it is to see $\\mu_1$ compared to $\\mu_2$ in the **target distribution** $p(\\mu\\mid y)$! In the Metropolis sampler, this ratio is used to accept or reject new samples $\\mu_{t+1}$ based on the previous sample $\\mu_t$. This approach can be integrated into the random-walk sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_g(y_bar, n, mu):\n",
    "    \"\"\"\n",
    "    Returns the joint density of y and mu assuming a normal likelihood and t prior,\n",
    "    at log scale (to prevent under/overflow issues)\n",
    "    \"\"\"\n",
    "    return n * (y_bar * mu - mu**2 / 2) - np.log(1 + mu**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_sampler(y, mu_init=0.0, sigma=1):\n",
    "    samples = 0  # numer of samples drawn so far\n",
    "    accept = 0  # number of accepted samples so far\n",
    "    mu = mu_init  # current state of sampler\n",
    "    y_bar = np.mean(y)\n",
    "    n = y.size\n",
    "    \n",
    "    while True:\n",
    "        yield (mu, accept/samples if samples > 0 else 0)\n",
    "        \n",
    "        candidate_mu = np.random.normal(loc=mu, scale=sigma)\n",
    "        acceptance_ratio = np.exp(log_g(y_bar, n, candidate_mu) - log_g(y_bar, n, mu))\n",
    "        \n",
    "        if np.random.uniform() < acceptance_ratio:\n",
    "            # accept the new candidate\n",
    "            mu = candidate_mu\n",
    "            accept += 1\n",
    "        \n",
    "        samples += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assume we observe the following month-over-month changes of mean exercise heart rate for 10 athletes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([3, 0, -2, 5, 1, 0, 8, -4, -1, 3])\n",
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = metropolis_sampler(y, sigma=5)\n",
    "samples = 1000\n",
    "\n",
    "large_step_trace = [next(sampler) for _ in range(samples)]\n",
    "np.mean([sample[0] for sample in large_step_trace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metropolis_trace = go.FigureWidget(\n",
    "    data=[go.Scatter(x=list(range(samples)), y=[sample[0] for sample in large_step_trace], name=f'large steps, acceptance rate = {large_step_trace[-1][1]:.3f}')]\n",
    ")\n",
    "\n",
    "metropolis_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = metropolis_sampler(y, sigma=0.05)\n",
    "small_step_trace = [next(sampler) for _ in range(samples)]\n",
    "metropolis_trace.add_scatter(x=list(range(samples)), y=[sample[0] for sample in small_step_trace], name=f'small steps, acceptance rate = {small_step_trace[-1][1]:.3f}');\n",
    "np.mean([sample[0] for sample in small_step_trace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = metropolis_sampler(y, sigma=1.2)\n",
    "good_step_trace = [next(sampler) for _ in range(samples)]\n",
    "metropolis_trace.add_scatter(x=list(range(samples)), y=[sample[0] for sample in good_step_trace], name=f'good step size, acceptance rate = {good_step_trace[-1][1]:.3f}');\n",
    "np.mean([sample[0] for sample in good_step_trace])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
