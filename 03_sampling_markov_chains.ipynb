{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Law of Large Numbers and Monte Carlo Estimation\n",
    "\n",
    "What if there's no well-defined PDF for our product of likehood and prior(s)? It means we cannot use `scipy.stats.norm`, and its convenient `mean()`, `pdf()`, `cdf()`, or `ppf()` functions to establish probabilistic answers to questions about the posterior. Is it possible to answer such questions in a different way? Assuming we have a sample $y_1,\\ldots,y_n$ from an _unknown_ distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.random.normal(loc=130, scale=10, size=10)  # although using samples from a normal distribution, pretend to not know the true distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is not defined by a PDF, but by a **sample**. According to the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), it is possible to approximate the distribution mean with the sample mean, with large enough sample. Is $n=10$ good enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As [mentioned before](./notebooks/01_bayes_rule_intro.ipynb#Continuous:), the probability of $Y$ being between e.g. 110 and 130 is\n",
    "\n",
    "$$Pr(110 \\le Y \\le 130) = \\int_{110}^{130}p(y)$$\n",
    "\n",
    "However, $p(y)$ is undefined and the only thing we have is a sample. But the sample can be used to approximate the integral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cdf(x, sample):\n",
    "    return (sample <= x).sum() / sample.size  # \"integrating\" by counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cdf(130, y) - sample_cdf(110, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be compared with the probability obtained from the true normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "unknown_norm = norm(loc=130, scale=10)\n",
    "unknown_norm.cdf(130) - unknown_norm.cdf(110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples also allow us to compute quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppf(p, sample):\n",
    "    p_index = int(np.round((sample.size - 1) * p))\n",
    "    return np.sort(sample)[p_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the range of $y$ for which the probability is 80% that it contains $y$'s true mean is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'[{sample_ppf(0.1, y)} - {sample_ppf(0.9, y)}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the interval from the true normal distribution would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'[{unknown_norm.ppf(0.1)} - {unknown_norm.ppf(0.9)}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a larger sample leads to more accurate estimations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_large = np.random.normal(loc=130, scale=10, size=10000)\n",
    "\n",
    "{\n",
    "    'mean (130)': np.mean(y_large),\n",
    "    'std (10)': np.std(y_large),\n",
    "    'p_110_130 (0.4772)': sample_cdf(130, y_large) - sample_cdf(110, y_large),\n",
    "    '80% ([117.2 - 142.8])': f'[{sample_ppf(0.1, y_large)} - {sample_ppf(0.9, y_large)}]'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process of computing means, variances, probabilities, percentiles (or any other quantities of interest) from simulated samples is called **Monte Carlo Estimation**. When using more complex models, simulations such as above can also be **chained**. For example, consider a model similar to the one in the [previous note](./02_generative_models.ipynb#Example,-observing-a-single-value-with-a-simple-model):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 2em\">\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu &\\sim\\color{red}{\\mathcal{t}(\\nu, m_0, s_0)}\\,\\mathrm{(prior)}\\\\\n",
    "y &\\sim\\color{blue}{\\mathcal{N}(\\mu, \\sigma^2_0)}\\,\\mathrm{(likelihood)}\\\\\n",
    "\\nu &=(n-1)\\,\\text{degrees of freedom for a sample size of}\\,n\\\\\n",
    "m_0 &=130\\\\\n",
    "s_0 &=10\\\\\n",
    "\\sigma^2_0 &=100\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint density $p(y, \\mu) = \\color{blue}{p(y\\mid\\mu)}\\color{red}{p(\\mu)}$ cannot be expressed as a known family (e.g. Normal) of probability densities, but can be estimated by simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# basing the prior on a sample of n=10, simulating m=1000 samples\n",
    "mu = t.rvs(df=9, loc=130, scale=10, size=1000)\n",
    "\n",
    "# an array of n location parameters can be plugged in to get n new samples\n",
    "y_chained = norm.rvs(loc=mu, scale=10)\n",
    "\n",
    "{\n",
    "    'mean (130)': np.mean(y_chained),\n",
    "    'std (10)': np.std(y_chained),\n",
    "    'p_110_130 (0.4772)': sample_cdf(130, y_chained) - sample_cdf(110, y_chained),\n",
    "    '80% ([117.2 - 142.8])': f'[{sample_ppf(0.1, y_chained)} - {sample_ppf(0.9, y_chained)}]'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are samples from the **prior predictive** distribution, i.e. given the likelihood and all the priors in a model, what values of $y$ can we expect (before having observed any data)?\n",
    "\n",
    "- Can we use the above approach to sample from the _posterior_ distribution $p(\\mu\\mid y)$? Why (not)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
